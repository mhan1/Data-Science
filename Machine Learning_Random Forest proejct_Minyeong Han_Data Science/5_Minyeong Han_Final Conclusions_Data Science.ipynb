{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Conclusion\n",
    "* Compare random forest modeling with linear regression modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Evaluate and Draw Conclusions from Random Forest Model\n",
    "- Once you have coerced and generated features, and selected features, it is time to fit the model, and score the model against a holdout set.\n",
    "\n",
    "After scoring the model, write (2 - 3) paragraphs where you interpret the model (by feature scores, and grouping features like geographic features, or datetimes), identify assumptions of your work (eg. omitted variable bias, multicollinearity), answer the \"so what\" of your analysis, and propose next steps.\n",
    "\n",
    "#### 2. Compare against linear regression\n",
    "How did the random forest model compare to the linear regression model?\n",
    "\n",
    "- If there is a significant difference in accuracy, what would account for this?\n",
    "- Is there a significant difference in the feature importances found?\n",
    "- Imagine that your company/stakeholder now needs to decide which type of model to rely on -- a linear regression model, or a random forests model. Which of the two models do you trust more? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluating the final Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#Loading the selected dataset with 10 features in the previous task.\n",
    "\n",
    "X_train = pd.read_feather('./X_train') \n",
    "y_train = pd.read_feather('./y_train')\n",
    "\n",
    "X_val = pd.read_feather('./X_val')\n",
    "y_val = pd.read_feather('./y_val')\n",
    "\n",
    "X_test = pd.read_feather('./X_test')\n",
    "y_test = pd.read_feather('./y_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.drop(columns = 'index', inplace=True)\n",
    "y_val.drop(columns = 'index', inplace=True)\n",
    "y_test.drop(columns = 'index', inplace=True)\n",
    "X_test.drop(columns = 'index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train['normalized_losses']\n",
    "y_val = y_val['normalized_losses']\n",
    "y_test = y_test['normalized_losses']\n",
    "X_test = X_test[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(131, 10) (33, 10) (41, 10)\n",
      "(131,) (33,) (41,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['symboling', 'num_of_doors', 'wheel-base', 'length', 'width', 'height',\n",
       "       'peak_rpm', 'highway_mpg', 'price', 'make_other'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By performing feature selection using permutation importance and correlation anlysis and then focused feature engineering, I could narrow down 48 features to 10 features for the final model. Also, below is the final random forest regressor after conducting hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rfr_tuned = RandomForestRegressor(min_samples_leaf = 4,\n",
    "                                  n_estimators = 14,\n",
    "                                  max_features = 0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the model & Score the model against a holdout set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(164, 10)\n",
      "(164,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "combined_X = np.vstack((X_train, X_val))\n",
    "print(combined_X.shape)\n",
    "combined_y = np.concatenate((y_train, y_val))\n",
    "print(combined_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5166901930384304"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_tuned.fit(X_train, y_train)\n",
    "rfr_tuned.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5415616813159074"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr_tuned.fit(combined_X, combined_y)\n",
    "rfr_tuned.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My final model's accuracy score is 0.5167, and evaluated accuracy score against the test set is 0.5416. This means my model can predict the target variable with unseen dataset pretty well, meaning my model was not overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Interpret the model by feature scores or grouping features, identify assumptions of your work, \"so what\" analysis, propose next steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['symboling', 'num_of_doors', 'wheel-base', 'length', 'width', 'height',\n",
       "       'peak_rpm', 'highway_mpg', 'price', 'make_other'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>weight</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>symboling</td>\n",
       "      <td>0.352003</td>\n",
       "      <td>0.035547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>height</td>\n",
       "      <td>0.237157</td>\n",
       "      <td>0.043077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>peak_rpm</td>\n",
       "      <td>0.102802</td>\n",
       "      <td>0.055563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>length</td>\n",
       "      <td>0.091071</td>\n",
       "      <td>0.007083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>price</td>\n",
       "      <td>0.082163</td>\n",
       "      <td>0.020734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>wheel-base</td>\n",
       "      <td>0.075515</td>\n",
       "      <td>0.006757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>num_of_doors</td>\n",
       "      <td>0.072968</td>\n",
       "      <td>0.027162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>highway_mpg</td>\n",
       "      <td>0.053519</td>\n",
       "      <td>0.016581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>width</td>\n",
       "      <td>0.018479</td>\n",
       "      <td>0.027678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>make_other</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.004117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature    weight       std\n",
       "0     symboling  0.352003  0.035547\n",
       "1        height  0.237157  0.043077\n",
       "2      peak_rpm  0.102802  0.055563\n",
       "3        length  0.091071  0.007083\n",
       "4         price  0.082163  0.020734\n",
       "5    wheel-base  0.075515  0.006757\n",
       "6  num_of_doors  0.072968  0.027162\n",
       "7   highway_mpg  0.053519  0.016581\n",
       "8         width  0.018479  0.027678\n",
       "9    make_other  0.000889  0.004117"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "pmi = PermutationImportance(rfr_tuned).fit(X_val, y_val)\n",
    "eli5.explain_weights_df(pmi, feature_names = X_val.columns.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based above, symboling, height, peak_rpm, length, num_of_doors are the top five important features in my model. The 10 features selected do not appear to be correlated to each other. From these feature importances to predict the normalized losses, I can say that a person who is interested in buying a car may consider those features to reduce the normalized losses. Also, car manufactureres may consider these features, and car insurance company will be able to gauge the normalized losses based on these features when they get new customers. I want to propose that the next step should be finding the optimal values for each significant feature to reduce the normalized losses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compare Random Forest Model against Linear Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### - Significant difference in accuracy? in the feature importances found? Which model do you trust more? why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In linear regression model, I could narrow down to 21 features from 48 features, and in random forest model, I could narrow down to 10 features. The accuracy of the linear regression model was 0.1956, while the accuracy of random forest model is 0.5167. With smaller number of features, the random forest model perforemd better to predict the target variable than the linear regression model did with more features.<br>Regarding the feature importances, the linear regression model found 'fuel_type_is_gas', 'compression_ratio', 'engine_size', 'symboling', 'drive_wheels_rwd' as the top five significant features. Meanwhile, the random forest model found 'symboling', 'height', 'peak_rpm', 'length', 'price' as the top five significant features. I think this discrepancy occurred because the meachanisms to find the important features of the two models are different from each other.<br>In my opinion, I prefer random forest because linear regression has more limitations or requirements. First, all the variables should be transformed into numeric variables for the linear regression model, which takes more time and also can be cumbersome. Also, missing values and outliers should be taken care of before fitting the model for the linear regression model, as the relationship between the features and the target variable is global, which means there is only one line for all the data points. Hence, if I know that the relationship between the target variable and the explanatory variables (features) is linear, I will use linear regression model, but if I am not sure about the relationship, I will use random forest model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
