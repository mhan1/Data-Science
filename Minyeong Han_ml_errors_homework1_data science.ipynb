{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source of Error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lesson, we'll begin practice some of the techniques for building a simulation dataset.  \n",
    "\n",
    "The NYTimes is considering hiring us to predict the popularity of their articles.  We'd like to give them a sense of the predictions that we could make even before we get our hands on their data.  To do so we construct a sample dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare a method called `build_feature` that returns a feature vector.  The method takes an argument of `size`, which specifies the length, or dimension, of the vector returned.  It also has a second argument of `going_from` which specifies the range of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "# use numpy's randint feature\n",
    "\n",
    "def build_feature(size, going_from = []):\n",
    "    return np.random.randint(going_from[0], going_from[1], size=size) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([337, 535, 372, 945, 444, 429, 883, 808, 690, 581, 478, 576, 554,\n",
       "       657, 768, 552, 790, 968, 698, 862, 880, 515, 803, 778, 386, 441,\n",
       "       693, 307, 619, 834, 613, 813, 616, 509, 564, 953, 927, 731, 933,\n",
       "       756, 842, 371, 687, 754, 861, 613, 815, 797, 343, 888])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "word_counts = build_feature(50, [300, 1000])\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counts)\n",
    "# 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[337],\n",
       "       [535],\n",
       "       [372],\n",
       "       [945],\n",
       "       [444]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts = word_counts.reshape(-1, 1)\n",
    "word_counts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts.shape\n",
    "# (50, 1)\n",
    "# The feature word_counts should return a vector of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[337],\n",
       "       [535],\n",
       "       [372]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[0:3]\n",
    "# array([[337],\n",
    "#        [535],\n",
    "#        [372]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the targets variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we are told that the true model, which determines the number of clicks on a news article, is the following: \n",
    "\n",
    "$y = \\theta_1x_1 + \\theta_0$\n",
    "\n",
    "where:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $y$ is a vector that contains the actual clicks on each article\n",
    "* $x_1$ is a vector of word counts of each article and \n",
    "* $\\theta_0$ represents the baseline number of clicks that an article receives (regardless of the word count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also provided the following:\n",
    "\n",
    "* $\\theta_1 = 75$\n",
    "* $\\theta_0 = 10000$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `word_counts` vector that we constructed above, and the defined parameter values, create our vector of target variables given the true model defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: break the problem into steps, examine the hstack method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[337],\n",
       "       [535],\n",
       "       [372],\n",
       "       [945],\n",
       "       [444]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biases = np.ones(50).reshape(-1, 1)\n",
    "biases[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[337.,   1.],\n",
       "       [535.,   1.],\n",
       "       [372.,   1.],\n",
       "       [945.,   1.],\n",
       "       [444.,   1.]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.hstack((word_counts, biases))\n",
    "features[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   75, 10000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = np.array([75, 10000])\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(features @ params).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "clicks = features @ params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35275., 50125., 37900., 80875., 43300., 42175., 76225., 70600.,\n",
       "       61750., 53575., 45850., 53200., 51550., 59275., 67600., 51400.,\n",
       "       69250., 82600., 62350., 74650., 76000., 48625., 70225., 68350.,\n",
       "       38950., 43075., 61975., 33025., 56425., 72550., 55975., 70975.,\n",
       "       56200., 48175., 52300., 81475., 79525., 64825., 79975., 66700.,\n",
       "       73150., 37825., 61525., 66550., 74575., 55975., 71125., 69775.,\n",
       "       35725., 76600.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clicks\n",
    "# array([35275., 50125., 37900., 80875., 43300., 42175., 76225., 70600.,\n",
    "#        61750., 53575., 45850., 53200., 51550., 59275., 67600., 51400.,\n",
    "#        69250., 82600., 62350., 74650., 76000., 48625., 70225., 68350.,\n",
    "#        38950., 43075., 61975., 33025., 56425., 72550.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset, let's train our model on the constructed dataset.  Assign the model to the variable `model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remember to use set fit_intercept=False\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression(fit_intercept=False)\n",
    "\n",
    "model.fit(features, clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   75., 10000.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_\n",
    "# array([   75., 10000.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring irreducible error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we now have trained a model to discover the true feature parameters of the underlying model.  Let's now consider, more realistically, that the underlying model takes the following form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y = \\theta_1x_1 + \\theta_0 + \\epsilon_i$\n",
    "\n",
    "Where \n",
    "* $\\theta_1 = 75$\n",
    "* $\\theta_0 = 10000$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use numpy's `randint` function to create a vector of random errors that range between positive and negative 8000.  Use this to create a new vector of target variables called `noisy_clicks` of length 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5349, -7765,  4172, -2808, -7095,  2955,  -187, -5105, -2944,\n",
       "       -7856, -3775,  -249,  2989,  6844, -4538,  7641,  1394, -2604,\n",
       "       -2626,  4645, -5038, -5484,  7243,   444, -4438, -3236,    93,\n",
       "       -1458, -7438,  2820,  7575,   151, -4951, -7247,  1719,  3742,\n",
       "       -6112, -6890, -1715, -1879,  7753, -6969, -3585, -5123, -4394,\n",
       "        1529, -5439,  6208,  4604,  6545])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "random_errors = np.random.randint(-8000, 8000, 50)\n",
    "random_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40624., 42360., 42072., 78067., 36205., 45130., 76038., 65495.,\n",
       "       58806., 45719., 42075., 52951., 54539., 66119., 63062., 59041.,\n",
       "       70644., 79996., 59724., 79295., 70962., 43141., 77468., 68794.,\n",
       "       34512., 39839., 62068., 31567., 48987., 75370., 63550., 71126.,\n",
       "       51249., 40928., 54019., 85217., 73413., 57935., 78260., 64821.,\n",
       "       80903., 30856., 57940., 61427., 70181., 57504., 65686., 75983.,\n",
       "       40329., 83145.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_clicks = clicks + random_errors\n",
    "noisy_clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([40624., 42360., 42072., 78067., 36205., 45130., 76038., 65495.,\n",
       "       58806., 45719.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_clicks[0:10]\n",
    "# array([40624., 42360., 42072., 78067., 36205., 45130., 76038., 65495.,\n",
    "#        58806., 45719.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script><script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window._Plotly) {require(['plotly'],function(plotly) {window._Plotly=plotly;});}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "linkText": "Export to plot.ly",
        "plotlyServerURL": "https://plot.ly",
        "showLink": false
       },
       "data": [
        {
         "mode": "markers",
         "name": "data",
         "text": [],
         "type": "scatter",
         "uid": "01fbb16a-72db-4bf6-9c85-16fd95b505ee",
         "x": [
          337,
          535,
          372,
          945,
          444,
          429,
          883,
          808,
          690,
          581,
          478,
          576,
          554,
          657,
          768,
          552,
          790,
          968,
          698,
          862,
          880,
          515,
          803,
          778,
          386,
          441,
          693,
          307,
          619,
          834,
          613,
          813,
          616,
          509,
          564,
          953,
          927,
          731,
          933,
          756,
          842,
          371,
          687,
          754,
          861,
          613,
          815,
          797,
          343,
          888
         ],
         "y": [
          40624,
          42360,
          42072,
          78067,
          36205,
          45130,
          76038,
          65495,
          58806,
          45719,
          42075,
          52951,
          54539,
          66119,
          63062,
          59041,
          70644,
          79996,
          59724,
          79295,
          70962,
          43141,
          77468,
          68794,
          34512,
          39839,
          62068,
          31567,
          48987,
          75370,
          63550,
          71126,
          51249,
          40928,
          54019,
          85217,
          73413,
          57935,
          78260,
          64821,
          80903,
          30856,
          57940,
          61427,
          70181,
          57504,
          65686,
          75983,
          40329,
          83145
         ]
        }
       ],
       "layout": {}
      },
      "text/html": [
       "<div id=\"8180634f-b5e2-4826-b320-fe0dc9f09bd6\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"8180634f-b5e2-4826-b320-fe0dc9f09bd6\")) {\n",
       "    Plotly.newPlot(\"8180634f-b5e2-4826-b320-fe0dc9f09bd6\", [{\"mode\": \"markers\", \"name\": \"data\", \"text\": [], \"x\": [337, 535, 372, 945, 444, 429, 883, 808, 690, 581, 478, 576, 554, 657, 768, 552, 790, 968, 698, 862, 880, 515, 803, 778, 386, 441, 693, 307, 619, 834, 613, 813, 616, 509, 564, 953, 927, 731, 933, 756, 842, 371, 687, 754, 861, 613, 815, 797, 343, 888], \"y\": [40624.0, 42360.0, 42072.0, 78067.0, 36205.0, 45130.0, 76038.0, 65495.0, 58806.0, 45719.0, 42075.0, 52951.0, 54539.0, 66119.0, 63062.0, 59041.0, 70644.0, 79996.0, 59724.0, 79295.0, 70962.0, 43141.0, 77468.0, 68794.0, 34512.0, 39839.0, 62068.0, 31567.0, 48987.0, 75370.0, 63550.0, 71126.0, 51249.0, 40928.0, 54019.0, 85217.0, 73413.0, 57935.0, 78260.0, 64821.0, 80903.0, 30856.0, 57940.0, 61427.0, 70181.0, 57504.0, 65686.0, 75983.0, 40329.0, 83145.0], \"type\": \"scatter\", \"uid\": \"d0334f33-cbcb-46f3-8523-30b77cf52316\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"8180634f-b5e2-4826-b320-fe0dc9f09bd6\")) {window._Plotly.Plots.resize(document.getElementById(\"8180634f-b5e2-4826-b320-fe0dc9f09bd6\"));};})</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<div id=\"8180634f-b5e2-4826-b320-fe0dc9f09bd6\" style=\"height: 525px; width: 100%;\" class=\"plotly-graph-div\"></div><script type=\"text/javascript\">require([\"plotly\"], function(Plotly) { window.PLOTLYENV=window.PLOTLYENV || {};window.PLOTLYENV.BASE_URL=\"https://plot.ly\";\n",
       "if (document.getElementById(\"8180634f-b5e2-4826-b320-fe0dc9f09bd6\")) {\n",
       "    Plotly.newPlot(\"8180634f-b5e2-4826-b320-fe0dc9f09bd6\", [{\"mode\": \"markers\", \"name\": \"data\", \"text\": [], \"x\": [337, 535, 372, 945, 444, 429, 883, 808, 690, 581, 478, 576, 554, 657, 768, 552, 790, 968, 698, 862, 880, 515, 803, 778, 386, 441, 693, 307, 619, 834, 613, 813, 616, 509, 564, 953, 927, 731, 933, 756, 842, 371, 687, 754, 861, 613, 815, 797, 343, 888], \"y\": [40624.0, 42360.0, 42072.0, 78067.0, 36205.0, 45130.0, 76038.0, 65495.0, 58806.0, 45719.0, 42075.0, 52951.0, 54539.0, 66119.0, 63062.0, 59041.0, 70644.0, 79996.0, 59724.0, 79295.0, 70962.0, 43141.0, 77468.0, 68794.0, 34512.0, 39839.0, 62068.0, 31567.0, 48987.0, 75370.0, 63550.0, 71126.0, 51249.0, 40928.0, 54019.0, 85217.0, 73413.0, 57935.0, 78260.0, 64821.0, 80903.0, 30856.0, 57940.0, 61427.0, 70181.0, 57504.0, 65686.0, 75983.0, 40329.0, 83145.0], \"type\": \"scatter\", \"uid\": \"d0334f33-cbcb-46f3-8523-30b77cf52316\"}], {}, {\"showLink\": false, \"linkText\": \"Export to plot.ly\", \"plotlyServerURL\": \"https://plot.ly\"}); \n",
       "}\n",
       "});</script><script type=\"text/javascript\">window.addEventListener(\"resize\", function(){if (document.getElementById(\"8180634f-b5e2-4826-b320-fe0dc9f09bd6\")) {window._Plotly.Plots.resize(document.getElementById(\"8180634f-b5e2-4826-b320-fe0dc9f09bd6\"));};})</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# if you have the plotly jupyter lab extension loaded, try the following\n",
    "from graph import trace_values, plot\n",
    "noisy_customers_trace = trace_values(word_counts.reshape(50), noisy_clicks)\n",
    "plot([noisy_customers_trace])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a model that has discovered the parameters of our model:\n",
    "\n",
    "$y = \\theta_1x_1 + \\theta_0 + \\epsilon_i$\n",
    "\n",
    "Where \n",
    "* $\\theta_1 = 75$\n",
    "* $\\theta_0 = 10000$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, that $\\epsilon_i$ represents our irreducible error, that is inherent to the future outcomes we try to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see this.  Once again, these are the parameters of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   75., 10000.])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_\n",
    "# array([   75., 10000.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these parameters along with our features to predict our data.  Use the sklearn's `predict` function in the linear regression class to make the predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35275., 50125., 37900., 80875., 43300., 42175., 76225., 70600.,\n",
       "       61750., 53575., 45850., 53200., 51550., 59275., 67600., 51400.,\n",
       "       69250., 82600., 62350., 74650., 76000., 48625., 70225., 68350.,\n",
       "       38950., 43075., 61975., 33025., 56425., 72550., 55975., 70975.,\n",
       "       56200., 48175., 52300., 81475., 79525., 64825., 79975., 66700.,\n",
       "       73150., 37825., 61525., 66550., 74575., 55975., 71125., 69775.,\n",
       "       35725., 76600.])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model.predict(features)\n",
    "predictions\n",
    "# array([35275., 50125., 37900., 80875., 43300., 42175., 76225., 70600.,\n",
    "#        61750., 53575., 45850., 53200., 51550., 59275., 67600., 51400.,\n",
    "#        69250., 82600., 62350., 74650., 76000., 48625., 70225., 68350.,\n",
    "#        38950., 43075., 61975., 33025., 56425., 72550., 55975., 70975.,\n",
    "#        56200., 48175., 52300., 81475., 79525., 64825., 79975., 66700.,\n",
    "#        73150., 37825., 61525., 66550., 74575., 55975., 71125., 69775.,\n",
    "#        35725., 76600.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Show that these predictions are the same that we get from matrix vector multiplication: $h(x) = X \\cdot \\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([35275., 50125., 37900., 80875., 43300., 42175., 76225., 70600.,\n",
       "       61750., 53575., 45850., 53200., 51550., 59275., 67600., 51400.,\n",
       "       69250., 82600., 62350., 74650., 76000., 48625., 70225., 68350.,\n",
       "       38950., 43075., 61975., 33025., 56425., 72550., 55975., 70975.,\n",
       "       56200., 48175., 52300., 81475., 79525., 64825., 79975., 66700.,\n",
       "       73150., 37825., 61525., 66550., 74575., 55975., 71125., 69775.,\n",
       "       35725., 76600.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_vector_predictions = features @ params\n",
    "matrix_vector_predictions\n",
    "# array([35275., 50125., 37900., 80875., 43300., 42175., 76225., 70600.,\n",
    "#        61750., 53575., 45850., 53200., 51550., 59275., 67600., 51400.,\n",
    "#        69250., 82600., 62350., 74650., 76000., 48625., 70225., 68350.,\n",
    "#        38950., 43075., 61975., 33025., 56425., 72550., 55975., 70975.,\n",
    "#        56200., 48175., 52300., 81475., 79525., 64825., 79975., 66700.,\n",
    "#        73150., 37825., 61525., 66550., 74575., 55975., 71125., 69775.,\n",
    "#        35725., 76600.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's use linear algebra to calculate the RSS of our model applied to predicting our noisy_clicks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5349., -7765.,  4172., -2808., -7095.,  2955.,  -187., -5105.,\n",
       "       -2944., -7856., -3775.,  -249.,  2989.,  6844., -4538.,  7641.,\n",
       "        1394., -2604., -2626.,  4645., -5038., -5484.,  7243.,   444.,\n",
       "       -4438., -3236.,    93., -1458., -7438.,  2820.,  7575.,   151.,\n",
       "       -4951., -7247.,  1719.,  3742., -6112., -6890., -1715., -1879.,\n",
       "        7753., -6969., -3585., -5123., -4394.,  1529., -5439.,  6208.,\n",
       "        4604.,  6545.])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = clicks + random_errors\n",
    "expected = predictions\n",
    "\n",
    "actual_minus_expected = actual - expected\n",
    "actual_minus_expected\n",
    "# array([ 5349., -7765.,  4172., -2808., -7095.,  2955.,  -187., -5105.,\n",
    "#        -2944., -7856., -3775.,  -249.,  2989.,  6844., -4538.,  7641.,\n",
    "#         1394., -2604., -2626.,  4645., -5038., -5484.,  7243.,   444.,\n",
    "#        -4438., -3236.,    93., -1458., -7438.,  2820.,  7575.,   151.,\n",
    "#        -4951., -7247.,  1719.,  3742., -6112., -6890., -1715., -1879.,\n",
    "#         7753., -6969., -3585., -5123., -4394.,  1529., -5439.,  6208.,\n",
    "#         4604.,  6545.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate rss using matrix algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1203968931.0"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss = actual_minus_expected @ actual_minus_expected\n",
    "rss\n",
    "# 1203968931.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we can calculate or mean squared error, simply by dividing the residual sum of squares by the number of predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24079378.62"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse = (rss/len(predictions))\n",
    "mse\n",
    "# 24079378.62"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we could have just used the sklearn library to calculate this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24079378.619999994"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(noisy_clicks, predictions)\n",
    "# 24079378.619999994"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if you remember RSS, RSS squares the error and then adds up these errors.  So, with mean squared error, we now just take the average of RSS.  But it's still hard to interpret squared error.  A more understandable metric is the root mean squared error, which is just the square root of the average squared error.  We'll calculate it below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4907.074344250349"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "sqrt(mean_squared_error(predictions, noisy_clicks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this says that on average, we are off by 4907 when we try to predict the noisy clicks.  This makes sense, as this is close to the average of our irreducible error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4307.26"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.average(abs(random_errors))\n",
    "#4307.26"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now produce a model that not only suffers from irreducible error, but also suffers from variance.  Use the `noisy_clicks` target variables and the previously defined `features` to produce this model.  Assign it to the variable `model_with_variance`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_variance = LinearRegression(fit_intercept = False).fit(features, noisy_clicks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=False, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  76.53450394, 8119.10474367])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_variance.coef_\n",
    "# array([  76.53450394, 8119.10474367])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So comparing with our first model, we see that our parameters no longer match the underlying values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   75., 10000.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting with our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have two models - one with irreducible error, and another that suffers from irreducible error and variance.  Let's compare these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already calculated the root mean squared error for our first model.  We saw that it fits the training set perfectly (or close to it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.203439287865707e-12"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "sqrt(mean_squared_error(predictions, clicks))\n",
    "#9.203439287865707e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate a new vector of target variables to see how our original model and our model with variance performs.  To do so, we'll first generate a new set of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6711,  -707,  4815, -6656,  3633,  -709,  1372,  2201, -3171,\n",
       "       -6480,  2141,  2234,  1224,  1289, -1600,  6826,  4328,  2742,\n",
       "        7780,  6707, -2352, -3548, -7761, -5557, -5898, -4584,  3627,\n",
       "        -710, -7426,  7905,  5512,    36,  1166,  6001,  3318,  4365,\n",
       "        7813,  4301, -1101, -7591, -6594, -7347, -4068, -6634,  4062,\n",
       "        7006,   281, -5300,  -511,  6879])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "new_errors = np.random.randint(-8000, 8000, 50)\n",
    "new_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the new errors to generate a new vector of observed data.  Data that neither model been trained with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28564., 49418., 42715., 74219., 46933., 41466., 77597., 72801.,\n",
       "       58579., 47095., 47991., 55434., 52774., 60564., 66000., 58226.,\n",
       "       73578., 85342., 70130., 81357., 73648., 45077., 62464., 62793.,\n",
       "       33052., 38491., 65602., 32315., 48999., 80455., 61487., 71011.,\n",
       "       57366., 54176., 55618., 85840., 87338., 69126., 78874., 59109.,\n",
       "       66556., 30478., 57457., 59916., 78637., 62981., 71406., 64475.,\n",
       "       35214., 83479.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_noisy_clicks = clicks + new_errors\n",
    "new_noisy_clicks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculate the root mean squared error for both the original model and the model with variance on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4921.427288907152"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_new_noisy_clicks_original_model = sqrt(mean_squared_error(new_noisy_clicks, predictions))\n",
    "rmse_new_noisy_clicks_original_model\n",
    "# 4921.427288907152"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([33911.232572  , 49065.06435244, 36589.94020995, 80444.21096851,\n",
       "       42100.42449375, 40952.40693463, 75699.07172413, 69958.98392851,\n",
       "       60927.91246339, 52585.65153376, 44702.59762777, 52202.97901405,\n",
       "       50519.21992733, 58402.27383332, 66897.60377084, 50366.15091945,\n",
       "       68581.36285756, 82204.50455917, 61540.18849493, 74091.84714135,\n",
       "       75469.4682123 , 47534.37427361, 69576.3114088 , 67662.94881026,\n",
       "       37661.42326514, 41870.82098193, 61157.51597522, 31615.19745375,\n",
       "       55493.96268354, 71948.88103099, 55034.75565989, 70341.65644821,\n",
       "       55264.35917171, 47075.16724996, 51284.56496675, 81056.48700004,\n",
       "       79066.58989756, 64065.827125  , 79525.79692121, 65979.18972354,\n",
       "       72561.15706252, 36513.40570601, 60698.30895157, 65826.12071566,\n",
       "       74015.31263741, 55034.75565989, 70494.7254561 , 69117.10438515,\n",
       "       34370.43959565, 76081.74424384])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_predictions = model_with_variance.predict(features)\n",
    "variance_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4984.958232759586"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_new_noisy_clicks_variance_model = sqrt(mean_squared_error(new_noisy_clicks, variance_predictions))\n",
    "rmse_new_noisy_clicks_variance_model\n",
    "# 4984.958232759586"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that our model did worse on this new dataset than it did with the data that it trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4824.297529089475"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "sqrt(mean_squared_error(noisy_clicks, variance_predictions))\n",
    "#4824.297529089475"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is generally the case.  When we train the data, our linear regression model is trying to fit to the training data.  And thus it finds parameters that match some of the randomness in the training data.  However, because this randomness is not replicated in future data, it generally performs less well on data it has not seen.  When seeing how well our model will perform, we want to see how well it performs on data it has not trained on, as this better simulates how the model will perform in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great job!  We used this lesson to review our teachings in linear algebra and numpy.  We also used it to develop our understanding of types of error: both variance and irreducible error.  We saw that if we only assess how well a model performs on the training set, this performance likely will not generalize to future data as our model has likely overfit to the training set.  To remedy this, we should assess how well a model performs on data it has not trained on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
